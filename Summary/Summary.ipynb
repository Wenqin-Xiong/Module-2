{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#           Yelp Data Analysis\n",
    "##            Wednesday_Group 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Introduction\n",
    "Given the dataset, we first clean the text data and change it into vectors, and extract features after building our own sentiment dictionary, then we try models such as linear regression, multinomial logistic regression, XGboost etc, in order to figure out what makes a review positive or negative and predict stars in the test dataset.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Background and Goal\n",
    "It is easy for human to understand whether a review is positive or negative through reading the text. However, when facing a large set of reviews, we need machines to do this for us. Therefore, the goal is to build models and then teach machine to do predictions of ratings as accurate as possible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Data Cleaning\n",
    "## 3.1 Tokenization\n",
    "All the reviews firstly are splitted into single words and then stored in the new dataset, while all the others like punctuations, whitespace and special symbols are removed. \n",
    "## 3.2 Transform to lower case\n",
    "Secondly, we change all the upper case in all of the words into lower case in order to treat them the same and count their frequency of use.\n",
    "## 3.3 Word stemming and lemmatization\n",
    "Thirdly, in order to recognize the same words which appear as different forms, we do word stemming and lemmatization. For example, 'playing' and 'played' are changed into the same as their shared root 'play'.\n",
    "## 3.4 Remove stop words \n",
    "At last, all the stop words like 'there', 'do', 'not' are removed since they are meanningless and helpless to the further analysing.\n",
    "## 3.5 Output\n",
    "Both training dataset and test dataset are dealed with all these data cleaning steps and after this we have the basement to do further analysis and model fitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. From text to variables\n",
    "## 4.1 Word to vector\n",
    "Firstly, by using R package 'text2vec', we count how many words are used in the whole training dataset and then treat each of them as a variable. For each cleaned review, the value becomes 0 if it does not cotain this word, otherwise the value is its count of use in this review. After every words in every review is assigned a value, we obtain a sparse matrix. The number of columns equals the number of words in total and the number of rows is the number of reviews.\n",
    "## 4.2 TF-IDF\n",
    "TF-IDF is a method that makes the change into the sparse matrix according to the frequecy of the words in the whole dataset. Although all the values that equal 0 in the sparse matrix will not be changed, other values in one review will be dealed with a weighted average based on its times of use in the whole dataset.\n",
    "## 4.3 Sentiment Dictionary\n",
    "We try to build a sentiment dictionary for words selected in LDA model. First, we use words with the clearest tendency to build a base for estimation.\n",
    "Let \n",
    "\n",
    "$$\\textrm{Positive%}_{word}=\\frac{\\textrm{appearance of word in 5&4-star reviews}}{\\textrm{appearance of word in all reviews}}$$\n",
    "$$\\textrm{Negative%}_{word}=\\frac{\\textrm{appearance of word in 1&2-star reviews}}{\\textrm{appearance of word in all reviews}}$$\n",
    "\n",
    "Then we set thresholds\n",
    "\n",
    "$$\\textrm{Positive threshold}= mean(\\textrm{Positive%}_{word})+sd(\\textrm{Positive%}_{word})$$ \n",
    "$$\\textrm{Negative threshold}= mean(\\textrm{Negative%}_{word})+sd(\\textrm{Negative%}_{word})$$ \n",
    "\n",
    "And select words with Positive% > Positive threshold to be positive words, and words with Negative% > Negative threshold to be negative. The intensity of sentiment is the corresponding Positive% or Negative%.\n",
    "Some words in base：\n",
    "<img src=\"t1.png\">\n",
    "And Positive threshold = 0.7511, Negative threshold = 0.3650, which means that the positive words have much higher intensities than the negative words.\n",
    "\n",
    "\n",
    "By using Word2vec method, we transform each word into a vector of 100 dimension. Then we denote the distance between two words by their cosine similarity\n",
    "\n",
    "$$Distance_{(a, b)}=\\frac{<a, b>}{||a||·||b||}$$\n",
    "\n",
    "For a “foreign” word not in the base, we select 20 words closest to it in the base. We then compute a weighted average of the sentiment of these 20 words and assign it to the “foreign” word. The weight is used to compensate the different proportions and intensities between positive and negative words, and the final formula is \n",
    "\n",
    "$$Sum_{word, p}=\\textrm{sum of the Intensity of all positive words selected}$$\n",
    "$$Sum_{word, n}=\\textrm{sum of the Intensity of all negative words selected}$$\n",
    "$$mean_p=\\textrm{mean Intensity of positive words in the base}$$\n",
    "$$mean_n=\\textrm{mean Intensity of negative words in the base}$$\n",
    "$$λ=\\textrm{proportion of positive words in the base}$$\n",
    "\n",
    "\n",
    "$$Intensity_{word}=\\frac{Sum_{word, p}/mean_p/λ-Sum_{word, n}/mean_n/(1-λ)}{2}$$\n",
    "\n",
    "Then we add the “foreign” word into the base and randomly select another “foreign” word. The proportions and mean intensities of positive and negative words are updated each iteration, but positive words eventually count for about 60% of all the words in dictionary.\n",
    "\n",
    "After all the words are added into the base, we again construct thresholds\n",
    "\n",
    "$$\\textrm{Positive threshold}= mean(\\textrm{Intensity}_\\textrm{positive word})+sd(\\textrm{Intensity}_\\textrm{positive word})$$ \n",
    "$$\\textrm{Negative threshold}= mean(\\textrm{Intensity}_\\textrm{negative word})-sd(\\textrm{Intensity}_\\textrm{negative word})$$ \n",
    "\n",
    "And select a new base. Then we repeat all the work again to make sure the dictionary is robust. Since the proportions and mean intensities of positive and negative words are quite similar in two versions of dictionary，we conclude that the algorithm is convergent.\n",
    "\n",
    "Some interesting words from the dictionary.\n",
    "<img src=\"t2.png\">\n",
    "$$Positive$$\n",
    "<img src=\"t3.png\">\n",
    "$$Negative$$\n",
    "\n",
    "In most people's mind, the taste like home is a great attraction, especially for those lives far from their home town. That why word related with home have strong positive sentiment. Also, people often describe themselves as picky in order to show the great pleasure the restaurant have brought to them.\n",
    "\n",
    "On the other hand, people do care about money, even words like \"discount\" and \"giftcard\" has strong negative sentiment. It seems that when people start to talk about money, they are no way to be satisfied. Surprisingly, most material words appear to be negative, it seems that people care about what kind of containers are used to serve their meal.   \n",
    "\n",
    "## 4.4 Latent dirichlet allocation\n",
    "1. Firstly we use the \"FindTopicNumber\" function in package \"topicmodels\" to obtain 15 topics as the best selection of topics' number based on 4 metrics: Griffths 2004, Caojuan2009, Arun 2010, Deveaud2014. \n",
    "\n",
    "2. Then we use LDA function in package\"ldatuning\" to extract 15 topics and for method, we choose \"Gibbs\".\n",
    "\n",
    "3. After we did LDA, we obtained a table of LDA to high-frequency words and their beta coefficients. Multiplying the frequency of each word in each review and the sentiment score of each word in our self-created sentiment dictionary, and then summing them up, we obtained each features' value in each review as the independent value. \n",
    "\n",
    "4. With these values of 15 features in dirrefent reviews, we can then do regression about the dependent variable y.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following plot shows how to determine the best number of topics. Based on minimizing \"Griffths 2004\" and \"Caojuan 2009\" as well as maximizing \"Arun 2010\" and \"Deveaud 2014\", we choose 15 as the number of topics according to the plot.\n",
    "<img src=\"figure.jpg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following plot shows the words content in each topic according to their decreasing beta coefficients.\n",
    "<img src=\"list.jpg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Models Fitting and Prediction\n",
    "## 5.1 Linear regression\n",
    "To begin with, we naively choose variables of the top 1000 most frequent used words to fit a linear regression model. \n",
    "\n",
    "This figure below shows some words that has significant effect to a review. It is obvious that words like worse, horrible awful will definitly hurt a reviw.  \n",
    "\n",
    "What's more, we concern about the other factors like location, length of a review. By fitting linear regression, it turns to be statistical significant bewteen the stars and the length of a review that before cleaned, while not significant between stars and the city. Thus, the variable that the length of reviews is added into model.\n",
    "With this linear regression model, we predict the stars of test dataset, the mse turns out to be over 0.7 which is not good enough.\n",
    "<img src=\"words1.jpg\">\n",
    "\n",
    "## 5.2 Multinomial logistic regression\n",
    "We try function 'multinom' in 'nnet' package based on variables selected by the previous linear regression model, to fit a multinomial logistic regression model since the response variable 'stars' is not binomial but has level more than 2. Firslt, we choose only 10 words with top 10 lowest p-values to be predictors, and then 20, 50, 100, 150, 180, 200... At last, we find that the mse of the cross validation by training dataset will not be smaller a lot when we keep number of variables over 180. Therefore, we finally keep 180 variables of words in our model.\n",
    "\n",
    "After the multinomial logistic regression model is build, we do prediction for training dataset by using two different prediction type: 'class' and 'probs'. When applying 'class', it gives the exact outcomes to be 1 or 2 or 3 or 4 or 5. And 'probs' means that it returns the probability values to predict the result as 1 to 5. And then we do weighted average to obtain our final predictions of 'probs' type. It turns out to be that type of 'probs' is much more accurate than type of 'class' and the mse is quite low.\n",
    "<img src=\"words2.jpg\">\n",
    "Above are some top significant words (changed from baseline). For example, the first one 'gem' is of great effect on rating, which we can understand from its literal meaning. Howeverm the word like 'ist' seems to be unlikely to appear here, which may caused by problems in cleanning.\n",
    "\n",
    "Therefore, we use this model to predict stars in training data. The result turns to be acceptable (having lowest rmse among all models we tried), which is exact our final model.\n",
    "\n",
    "## 5.3 XGBoost\n",
    "\n",
    "In this part, we set the parameter 'nround' which means times of iteration to be 200 and the 'etc' which means the reduction of rmse in each iteration to be 0.01. Actually, rmse does not converge at last. Therefore, we add more and more the 'nrounds' and corresponding higher 'etc' and finally the rmse does not become lower any more in the last several iterations when 'nrounds' setted 500. \n",
    "\n",
    "## 5.4 Models rebuilding via features from LDA \n",
    "We repeat our model building process using features instead of words. However, the results are not as good as what we did before.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Strengths and Weaknesses\n",
    "Stengths : The advantages of our final model is easy to interpret with higher accuracy in the cross validation of training dataset. What's more, we create sentimental dictionary by ourself and find out some words which have high sentiments but easy to be ignored like 'hometown'."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Weaknesses : We do not make full use of the whole data, for example, other information like time of reviews, categories are wasted. In addition, our final rmse is 0.68, not low enough campared to other groups. We consider it is because our prediction is right skewed and more likely to predict it positive, thus, for true values of 1 and 2 we have higher mse even if our accuracy is relative high."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Conclusion\n",
    "Having done all the process dealing with the Yelp dataset and model selection, we meet goals of this module. Our final model is multinomial logistic regression with 'probs' type of prediction which has good accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Contribution\n",
    "Data cleaning: Wenqin Xiong, Yiding Ding\n",
    "\n",
    "Feature extraction and dictionary: Wenqin Xiong, Yiding Ding\n",
    "\n",
    "Model building: Jianxiong Wang, Jin Tao\n",
    "\n",
    "Summary: Jianxiong Wang, Jin Tao, Wenqin Xiong, Yiding Ding\n",
    "\n",
    "Slide: Jianxiong Wang, Jin Tao, Wenqin Xiong, Yiding Ding"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.4.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
