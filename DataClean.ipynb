{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pandas\n",
    "import nltk\n",
    "import time\n",
    "import re\n",
    "import numpy as np\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(action='ignore', category=UserWarning, module='gensim')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'train_data.csv'\n",
    "t1 = time.time()\n",
    "data = pandas.read_csv('train_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "nreviews=10000\n",
    "#nreviews=data.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_txt=data[\"text\"]\n",
    "data_txt=data_txt[0:nreviews]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop = set(stopwords.words(\"english\"))\n",
    "stop_neg=[\"wasn't\",'wasn',\"couldn't\",'couldn',\n",
    "          \"wouldn't\",'wouldn',\"shouldn't\",'shouldn',\"shan't\",\n",
    "          \"haven't\",\"hasn't\",\"hadn't\",\"hadn'\",\n",
    "          \"don't\",'don',\"doesn't\",'doesn',\"didn't\",\n",
    "          \"needn't\",'needn','no','nor',\n",
    "          \"isn't\",'isn',\"aren't\",\"weren't\",'weren',\"won't\"]\n",
    "stop_revised=[w for w in stop if not w in stop_neg]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stop_delete(sentence):\n",
    "    sentence=sentence.lower().split(\" \")\n",
    "    sen=[word for word in sentence if not word in stop]\n",
    "    sen_re=np.array([])\n",
    "    for j in sen:\n",
    "        word_re=re.sub('[^a-zA-Z]','', j)\n",
    "        if word_re != \"\":\n",
    "            sen_re=np.append(sen_re,word_re)\n",
    "    return sen_re\n",
    "\n",
    "def stop_delete_re(sentence):\n",
    "    sentence=sentence.lower().split(\" \")\n",
    "    sen=[word for word in sentence if not word in stop_revised]\n",
    "    sen_re=np.array([])\n",
    "    for j in sen:\n",
    "        if j in stop_neg:\n",
    "            sen_re=np.append(sen_re,\"neg_w\")\n",
    "        else:\n",
    "            word_re=re.sub('[^a-zA-Z]','', j)\n",
    "            if word_re != \"\":\n",
    "                sen_re=np.append(sen_re,word_re)\n",
    "    return sen_re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from nltk.stem.wordnet import WordNetLemmatizer \n",
    "from nltk.stem.porter import PorterStemmer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "lem = WordNetLemmatizer()\n",
    "stem = PorterStemmer()\n",
    "\n",
    "#clear text\n",
    "document=[]\n",
    "document_re=[]\n",
    "for sen in data_txt:\n",
    "    txt=stop_delete(sen)\n",
    "    txt_re=stop_delete_re(sen)\n",
    "    \n",
    "    txt=[stem.stem(word) for word in txt]\n",
    "    txt_re=[stem.stem(word) for word in txt_re]\n",
    "    \n",
    "    document.append(txt)\n",
    "    document_re.append(txt_re)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#frequency, threshold=min_freq\n",
    "frequency=defaultdict(int)\n",
    "#intensity\n",
    "freq_pos=defaultdict(int)\n",
    "freq_neg=defaultdict(int)\n",
    "\n",
    "score=data[\"stars\"]\n",
    "\n",
    "for i in range(nreviews):\n",
    "    sen=document[i]\n",
    "    for word in sen:\n",
    "        frequency[word] += 1\n",
    "        if score[i] > 3:\n",
    "            freq_pos[word] += 1\n",
    "        elif score[i] < 3:\n",
    "            freq_neg[word] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_freq=100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "document=[[word for word in sen if frequency[word] > min_freq] for sen in document]\n",
    "document_re=[[word for word in sen if frequency[word] > min_freq] for sen in document_re]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "frequency_dict={}\n",
    "for word in list(frequency):\n",
    "    if word not in list(freq_pos):\n",
    "        freq_pos[word]=0\n",
    "    if word not in list(freq_neg):\n",
    "        freq_neg[word]=0    \n",
    "    if frequency[word] <= min_freq:\n",
    "        del frequency[word]\n",
    "        del freq_pos[word]\n",
    "        del freq_neg[word]\n",
    "    else:\n",
    "        frequency_dict[word]=[frequency[word],freq_pos[word],freq_neg[word]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_w=data[0:nreviews]\n",
    "data_w=data_w.drop(\"text\", axis=1)\n",
    "\n",
    "text=[]\n",
    "\n",
    "for i in range(nreviews):\n",
    "    sen=document_re[i]\n",
    "    txt=\"\"\n",
    "    for w in sen:\n",
    "        txt=txt+w+\" \"\n",
    "    text.append(txt)\n",
    "\n",
    "data_w[\"text\"]=text    \n",
    "        \n",
    "data_w.to_csv(\"data_cleaned.csv\", sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('document.csv', 'w', newline=\"\") as myfile:\n",
    "    writer = csv.writer(myfile)\n",
    "    writer.writerows(document)\n",
    "    \n",
    "with open('document_re.csv', 'w', newline=\"\") as myfile:\n",
    "    writer = csv.writer(myfile)\n",
    "    writer.writerows(document_re)\n",
    "\n",
    "with open('frequency.csv', 'w', newline=\"\") as myfile:\n",
    "    writer = csv.writer(myfile)\n",
    "    writer.writerow([\"word\",\"count\",\"pos\",\"neg\"])\n",
    "    for key, value in frequency_dict.items():\n",
    "        writer.writerow([key, value[0],value[1],value[2]])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
